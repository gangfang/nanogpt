{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Load the data set\n",
    "with open('input.txt', 'r') as f:\n",
    "  input = f.read()\n",
    "chars = sorted(list(set(input)))\n",
    "vocab_size = len(chars)\n",
    "print(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# simple tokenizor: mappings and encode and decode functions\n",
    "stoi = {s: i for i, s in enumerate(chars)}\n",
    "itos = {i: s for i, s in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# encode the entire text dataset\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "data = torch.tensor(encode(input))\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and val sets\n",
    "# learning: this split is simple, the entire encoded set is simply split into two.\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]], device='cuda:0')\n",
      "targets\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]], device='cuda:0')\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "# data loader\n",
    "\"\"\"\n",
    "learning: notice the difference between the organization of this data and that of makemore (i.e., name) data.\n",
    "The makemore data is simpler in that it consists of names, which structures the data with each name having an start and an end. \n",
    "A block, defined by the block_size, operates within a name. On the contrary, this shakespera data is a blob of \n",
    "text, there is no inherent structure to it and hence allows abitrary slicing. A block in this context operates \n",
    "in the entire text blob and the positioning of it is determined by a random process (i.e., randint()) since there is \n",
    "no natural start points.\n",
    "WAIT, the above analysis doesn't seem to be complete. Even though there is a natural start and end point of a name, \n",
    "the `def build_dataset(words):` function uses a block to slide through a word and repeats for all words. And the \n",
    "X in data contains examples of blocks instead of words. And then, I can use the same technique (i.e., block sliding)\n",
    "for the entire text blob in this data, producing a dataset of a size of (len(text) - block_size).\n",
    "So I guess they are different options for setting up the data.\n",
    "\n",
    "An input block is reused (see `when input is... the target: ...` logs) to max training opportunities.\n",
    "\n",
    "learning: torch.stack() turns\n",
    "[tensor([24, 43, 58,  5, 57,  1, 46, 43]),\n",
    "  tensor([44, 53, 56,  1, 58, 46, 39, 58]),\n",
    "  tensor([52, 58,  1, 58, 46, 39, 58,  1]),\n",
    "  tensor([25, 17, 27, 10,  0, 21,  1, 54])]\n",
    "into\n",
    "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
    "         [44, 53, 56,  1, 58, 46, 39, 58],\n",
    "         [52, 58,  1, 58, 46, 39, 58,  1],\n",
    "         [25, 17, 27, 10,  0, 21,  1, 54]])\n",
    "\"\"\"\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split, batch_size, block_size):\n",
    "  data = train_data if split == 'train' else val_data\n",
    "  idx = torch.randint(len(data)-block_size, (batch_size,))\n",
    "  # learning: I didn't use torch.stack\n",
    "  x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "  x, y = x.to(device), y.to(device)\n",
    "  return x, y\n",
    "\n",
    "xb, yb = get_batch('train', batch_size, block_size)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "# learning: the targets in yb are index into the next char\n",
    "print('targets')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "# note: this structure in the training data isn't needed for the bigram model training\n",
    "# note: this structure in the training data is only applied to the attention layers during the training of transformer model\n",
    "for b in range(batch_size):\n",
    "  for t in range(block_size):\n",
    "    # learning: I forgot the indexing of time dimension is `:t+1`\n",
    "    context = xb[b, :t+1]\n",
    "    target = yb[b, t]\n",
    "    print(f'when input is {context.tolist()} the target: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(5.0364, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "yq$;tfBfROkNdcuwdZZTkOMl;,ertK\n",
      "w:!PLCkMBbeA$3:XaSGJO-3p&M-c?KL3auhpFYVXJFhNNNuhq$OMxv.tbVFYdXlrFZaAe\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    # learning: this token_embedding_table is bound to a nn.Embedding object\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    # TODO: I need a positional embedding table\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    # learning: I didn't know this guy returns logits\n",
    "    # learning: self.token_embedding_table[idx] and got TypeError: 'Embedding' object is not subscriptable\n",
    "    # I forgot that nn.Embedding is a class and it takes inputs as described in https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "    logits = self.token_embedding_table(idx)\n",
    "\n",
    "    # got RuntimeError: Expected target size [4, 65], got [4, 8] when `loss = F.cross_entropy(logits, targets)`\n",
    "    # to fix that, we transform them into 2d tensors by merging the B and T dimensions using `.view`\n",
    "    if targets is not None:\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B*T, C)  # B*T shows T is useless outside of the attention module\n",
    "      targets = targets.view(B*T)\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "    else:\n",
    "      loss = None\n",
    "    \n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # learning: I need to start with the form of generation. In this case, the form is that of ChatGPT\n",
    "    # I provide a prompt (i.e., idx) and the model use that to generate texts of length max_new_tokens\n",
    "    for _ in range(max_new_tokens):\n",
    "      logits, _ = self(idx)\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      idx_next = torch.multinomial(probs, num_samples=1)\n",
    "      # learning: idx.append(idx_next) -> AttributeError: 'Tensor' object has no attribute 'append'\n",
    "      idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "m = m.to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "# learning: the out.shape is [4,8,65] because we haven't done multinomial on the 65 logits yet. Therefore,\n",
    "# we have the last dimension of 65\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "# Use `torch.zeros((1,1), dtype=torch.long)` as the prompt. `torch.long` is necessary because the default \n",
    "# dtype is float with `zeros`\n",
    "print(m.generate(torch.zeros((1,1), dtype=torch.long, device=device), 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wawice my.\n",
      "\n",
      "HDEdaromzy mug\n",
      "Yowhthmoof isth ble mil;KI ll!,\n",
      "\n",
      "W:\n",
      "\n",
      "Ye sengmin lat HNGEdrovDEs, and Win nghir.\n",
      "TWjomesel lind me l.\n",
      "HAshe ce hiry ptug; aisspllw y.\n",
      "Hllin's noroopetelives\n",
      "MPOFGll, d mothakleo Windo whthCoisb3MI'Tham dourive ce higend t so mower; te\n",
      "\n",
      "ANk d nterurt f s ar igr Wam:\n",
      "\n",
      "Enge maleronth,\n",
      "Mf Pre?\n",
      "\n",
      "WISo myr f-NLLERar,\n",
      "\n",
      "b&hak\n",
      "ardsal thes ghesthiuin ccuk?\n",
      "araney Iry ts I&fr y c!NGJknge tonok, mary.\n",
      "Yor 'Wour me?m sora anghy t-senomes twe men.\n",
      "Wand tho-z; cin s th llugy od,OThourc\n"
     ]
    }
   ],
   "source": [
    "# train the bigram model with torch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters())\n",
    "\n",
    "for steps in range(10000):\n",
    "  xb, yb = get_batch('train', batch_size, block_size)\n",
    "  logits, loss = m(xb, yb)\n",
    "\n",
    "  loss.backward()\n",
    "  # learning: I put zero_grad() before step() and no learning occurred. `.data -= lr * .grad`, nothing happens\n",
    "  # when `.grad` is 0\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "print(m.generate(torch.zeros((1,1), dtype=torch.long, device=device), 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9407,  1.6751],\n",
      "        [ 1.0907,  0.1475],\n",
      "        [-0.6920, -0.7834],\n",
      "        [-0.0246,  0.3170],\n",
      "        [ 0.8250,  0.4557],\n",
      "        [-0.8341, -0.8359],\n",
      "        [-0.8077,  0.9350],\n",
      "        [ 0.9412,  1.1077]])\n",
      "tensor([[-0.9407,  1.6751],\n",
      "        [ 0.0750,  0.9113],\n",
      "        [-0.1807,  0.3464],\n",
      "        [-0.1416,  0.3391],\n",
      "        [ 0.0517,  0.3624],\n",
      "        [-0.0959,  0.1627],\n",
      "        [-0.1976,  0.2730],\n",
      "        [-0.0553,  0.3773]])\n"
     ]
    }
   ],
   "source": [
    "# aggregate past context. The input and output share the same shape: (B, T, C)\n",
    "# note: there is a nested loop approach and a softmax approach to this\n",
    "# B,T,C = 4,8,2\n",
    "# x = torch.randn(B,T,C)\n",
    "# xbow = torch.zeros((B,T,C))\n",
    "# wei = torch.tril(torch.ones((T,T)))\n",
    "# aver = wei / torch.sum(wei, 1, keepdim=True)\n",
    "# xbow = aver @ x\n",
    "# print(x[0])\n",
    "# print(xbow[0])\n",
    "\n",
    "# the softmax approach\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # this wei is 2d\n",
    "aver = torch.softmax(wei, dim=-1)  # this is the \"attention pattern\" described in 1B3B video\n",
    "xbow = aver @ x # (T T) @ (T 2)\n",
    "print(x[0])\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern: \n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2946, 0.7054, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1027, 0.0692, 0.8281, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1805, 0.6161, 0.1524, 0.0510, 0.0000, 0.0000],\n",
      "        [0.3649, 0.2724, 0.0512, 0.2387, 0.0728, 0.0000],\n",
      "        [0.3559, 0.1576, 0.0735, 0.0998, 0.1328, 0.1805]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "value matrix before weighted sum (i.e., wei @ v):\n",
      "tensor([[ 4.3790e-01, -7.2814e-01,  7.8591e-01, -5.4940e-01,  5.5663e-01,\n",
      "         -4.8415e-01,  6.0563e-01, -2.2115e-01],\n",
      "        [ 2.4389e-01, -9.4100e-01, -6.5698e-02,  1.0840e+00, -3.3134e-01,\n",
      "          7.0538e-01,  1.6110e+00, -2.4243e-01],\n",
      "        [-1.7131e-01,  6.7616e-01, -4.2472e-01, -4.2271e-01,  5.0947e-02,\n",
      "         -4.3690e-01, -9.3460e-01, -5.0428e-02],\n",
      "        [ 8.2210e-01, -1.8570e-01, -7.9552e-01,  4.3803e-02, -9.9740e-01,\n",
      "         -4.9402e-01, -7.5507e-01,  1.0097e+00],\n",
      "        [ 1.9484e-01,  3.3190e-01,  3.3583e-02,  4.0934e-01,  7.2196e-04,\n",
      "          3.4829e-01,  2.5425e-01, -2.5071e-03],\n",
      "        [-7.7258e-01,  3.9385e-01, -5.2865e-02,  8.8265e-01,  2.9763e-01,\n",
      "          7.2813e-01,  3.3500e-01,  9.5112e-02]], grad_fn=<SelectBackward0>)\n",
      "output of the self attention head: \n",
      "tensor([[ 0.4379, -0.7281,  0.7859, -0.5494,  0.5566, -0.4841,  0.6056, -0.2212],\n",
      "        [ 0.3010, -0.8783,  0.1852,  0.6028, -0.0698,  0.3550,  1.3148, -0.2362],\n",
      "        [-0.0800,  0.4201, -0.2756, -0.3314,  0.0764, -0.3627, -0.6003, -0.0812],\n",
      "        [ 0.2452, -0.6176, -0.0039,  0.5065, -0.1468,  0.2554,  0.9209, -0.1455],\n",
      "        [ 0.4279, -0.5075,  0.0597,  0.1133, -0.1226, -0.0995,  0.4501,  0.0916],\n",
      "        [ 0.1502, -0.2611,  0.1537,  0.1622,  0.1039,  0.0351,  0.4196, -0.0031]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# turn the dumb aggregate method into a single head of self attention\n",
    "B,T,C = 4,6,8\n",
    "head_size = 8\n",
    "x = torch.randn(B,T,C)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "q = query(x) # B, T, head_size\n",
    "k = key(x) # B, T, head_size\n",
    "v = value(x) # B, T, head_size\n",
    "wei = q @ k.transpose(-2, -1)  # this wei is 3d. that is why softmax's dim can't be 1, but is -1\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = torch.softmax(wei, dim=-1) # B, T, T\n",
    "print('attention pattern: ')\n",
    "print(wei[0])\n",
    "print('value matrix before weighted sum (i.e., wei @ v):')\n",
    "print(v[0])\n",
    "out = wei @ v  # B, T, head_size\n",
    "print('output of the self attention head: ')\n",
    "print(out[0])  # I can see how the weighted sum affects the output of the attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# adding a linear layer to the bigram model\n",
    "# adding a positional embedding to the model\n",
    "pos_embedding_table = nn.Embedding(8, 32)\n",
    "pos_embedding_table(torch.arange(8))\n",
    "print(torch.allclose(pos_embedding_table.weight, pos_embedding_table(torch.arange(8))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention definition and model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "max_iters = 8000\n",
    "eval_iters = 200\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "dropout = 0.2\n",
    "\n",
    "class Head(nn.Module):\n",
    "  \"\"\" One head of self attention \"\"\"\n",
    "  def __init__(self, head_size, n_embd):\n",
    "    super().__init__()\n",
    "    self.n_embd = n_embd\n",
    "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "    # learning: I forgot to use register_buffer. I use it because I don't want it to be in the computational graph\n",
    "    self.register_buffer('tril', torch.tril(torch.ones((block_size, block_size))))\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B,T,C = x.shape\n",
    "    # learning: what I wrote vs. what is enough: B, T, C = x.shape[0], x.shape[1], x.shape[2]\n",
    "    q = self.query(x)\n",
    "    k = self.key(x)\n",
    "    v = self.value(x)\n",
    "    # learning: the normalization base is C, I used 2\n",
    "    wei = q @ k.transpose(-2, -1) * self.n_embd**-0.5\n",
    "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "    wei = torch.softmax(wei, dim=-1)\n",
    "    wei = self.dropout(wei)\n",
    "    out = wei @ v\n",
    "    return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  \"\"\" Multiple heads of self attention in parallel \"\"\"\n",
    "  def __init__(self, n_head, n_embd):\n",
    "    super().__init__()\n",
    "    head_size = n_embd // n_head\n",
    "    # self.h = Head(head_size)\n",
    "    # learning: the above was what I did. I should instead put these Heads in a ModuleList\n",
    "    self.heads = nn.ModuleList([Head(head_size, n_embd) for _ in range(n_head)])\n",
    "    self.proj = nn.Linear(head_size * n_head, n_embd, bias=False)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # return torch.cat((self.h(x), self.h(x), self.h(x), self.h(x)), dim=-1)\n",
    "    # learning: the above was what I did. I should instead put these Heads in a ModuleList\n",
    "    # Because this class is a representation of MULTI heads instead of a single head being forwarded multiple times\n",
    "    x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    x = self.proj(x)\n",
    "    x = self.dropout(x)\n",
    "    return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "  \"\"\" This layer is applied to each position in a block locally and independently \"\"\"\n",
    "  def __init__(self, n_embd):\n",
    "    super().__init__()\n",
    "    # learning: I forgot the non-linearity. With the non-linearity, I also need nn.Sequential\n",
    "    self.net = nn.Sequential(\n",
    "      nn.Linear(n_embd, n_embd, bias=False),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(n_embd, n_embd, bias=False),  # projection layer\n",
    "      nn.Dropout(dropout)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.net(x)\n",
    "  \n",
    "class Block(nn.Module):\n",
    "  \"\"\" Transformer block: Communicate-then-compute\"\"\"\n",
    "\n",
    "  def __init__(self, n_head, n_embd):\n",
    "    super().__init__()\n",
    "    self.sa = MultiHeadAttention(n_head, n_embd)\n",
    "    self.ffwd = FeedForward(n_embd=n_embd)\n",
    "    self.ln1 = nn.LayerNorm(n_embd)\n",
    "    self.ln2 = nn.LayerNorm(n_embd)    \n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = x + self.sa(self.ln1(x))\n",
    "    x = x + self.ffwd(self.ln2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.token_embd_table = nn.Embedding(vocab_size, n_embd)\n",
    "    self.pos_embd_table = nn.Embedding(block_size, n_embd)\n",
    "    self.blocks = nn.Sequential(*[Block(n_head, n_embd) for _ in range(n_layer)])\n",
    "    self.ln_f = nn.LayerNorm(n_embd)\n",
    "    self.fully_connected = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    B, T = idx.shape\n",
    "    token_embd = self.token_embd_table(idx) # B T C\n",
    "    # learning: I wrote self.pos_embd_table(idx). But the pos_embd_table takes POSITION as input instead of idx\n",
    "    # learning 2: note that torch.arange(T) uses T instead of block_size because idx might be shorter than block_size\n",
    "    pos_embd = self.pos_embd_table(torch.arange(T, device=device)) # B T C\n",
    "    x = token_embd + pos_embd\n",
    "    x = self.blocks(x) # B T C\n",
    "    x = self.ln_f(x)\n",
    "    logits = self.fully_connected(x) # B T vocab_size\n",
    "\n",
    "    if targets is not None: # training mode\n",
    "      logits = logits.view(B*T, -1)\n",
    "      # learning: I wrote targets.view(B*T, -1), which returns a 2d tensor. I need 1d for targets instead\n",
    "      targets = targets.view(B*T)\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "    else:\n",
    "      loss = None\n",
    "\n",
    "    return logits, loss\n",
    "\n",
    "\n",
    "  def generate(self, idx, max_token):\n",
    "    for _ in range(max_token):\n",
    "      # learning: forgot to crop idx\n",
    "      idx_cond = idx[:, -block_size:]\n",
    "      logits, _ = self(idx_cond)\n",
    "      # learning: forgot the line below. Logits are generated for each and every position in a sequence and \n",
    "      # I only need the last set of logits\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      next_token = torch.multinomial(probs, num_samples=1)\n",
    "      idx = torch.cat((idx, next_token), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5466689 parameters\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# learning: parameters can be extracted simply using m\n",
    "m = GPTLanguageModel()\n",
    "m = m.to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), learning_rate)\n",
    "# learning: m.parameters() returns an iterable of params, each of which are defined \n",
    "# using basic layer constructs provided in torch.nn\n",
    "print(sum(p.numel() for p in m.parameters()), 'parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st training run attempt - failed\n",
    "```\n",
    "OUTPUT\n",
    "      0/   5000: 4.7011\n",
    "    200/   5000: 4.7024\n",
    "    400/   5000: 4.5861\n",
    "    600/   5000: 4.8061\n",
    "    800/   5000: 4.7599\n",
    "   1000/   5000: 4.8209\n",
    "   1200/   5000: 4.9178\n",
    "   1400/   5000: 4.7532\n",
    "   1600/   5000: 4.8195\n",
    "   1800/   5000: 4.6576\n",
    "   2000/   5000: 4.8024\n",
    "   2200/   5000: 4.6016\n",
    "   2400/   5000: 4.6853\n",
    "   2600/   5000: 4.7858\n",
    "   2800/   5000: 4.8272\n",
    "   3000/   5000: 4.8139\n",
    "   3200/   5000: 4.7915\n",
    "   3400/   5000: 4.6659\n",
    "   3600/   5000: 4.7886\n",
    "   3800/   5000: 4.7374\n",
    "   4000/   5000: 4.7108\n",
    "   4200/   5000: 4.8204\n",
    "   4400/   5000: 4.8655\n",
    "   4600/   5000: 4.7810\n",
    "   4800/   5000: 4.6445\n",
    "\n",
    "bA&XUjnT$TZ?Nhe$Rz:bhYKcdQgNbYF?GOajb'pw?M-\n",
    ";w?SnYwPqKLQw?ydxodwcXcGbIYmNT,Y\n",
    "Kyp$VX'DgaNY ziDbSOiaX,aTq mKRs!y&'d\n",
    "I'etfyGaH\n",
    "XjJSGKYO,vQ&iUjch&mvmmZkaTTKWzXr\n",
    "bYF?BnIBOacdhWTYKVTSjmmuZj ;XVbYKeqzSYCWoYNYReOO$NOA.j\n",
    "\n",
    "qhahWXTYFxN!.Ya'kzq$qvmiqfoGnstMMcaOFf3SSaF;fRaKAOaGYFRGsWCFY?Iq!qfamN-WBy!AF!PHgXa$JUT;:&bSjOmKMKRYT3Cj RKWIaUWzeW$FOah&mxUATVmROtqh!&uKRgNdTXb;Tbs\n",
    ";MuT?&aAb;,YKRaXKmxc:eTX;GaHhWz,Oa$OOa\n",
    "Wc!UObGOGUXOTSrTTYUGMFYm;jWzYjaqOBNUJ&dFbWC-Tm\n",
    "WDTTjA\n",
    "isiOb;.WhqA\n",
    "TXON!wv\n",
    "IqeNIPUiXhinYsf.UN\n",
    "T, DmP\n",
    "```\n",
    "\n",
    "Two questions For the above training run:\n",
    "1. I don't think I need to Carefully initialize the Parameters like the Make more series anymore Because The definition of parameters in a layer is not done by hand. But what is the range of initial loss? I got above four.\n",
    "  AN: Andrej's: `step 0: train loss 4.4116, val loss 4.4022`. Same range.\n",
    "2. The training is not working Given that the loss is not decreasing. Why?\n",
    "  AN: I have the this in the previous cell..\n",
    "```\n",
    "optimizer = torch.optim.AdamW(m.parameters())\n",
    "m = GPTLanguageModel()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2937, val loss 4.2903\n",
      "step 500: train loss 2.1114, val loss 2.1642\n",
      "step 1000: train loss 1.7357, val loss 1.8782\n",
      "step 1500: train loss 1.5621, val loss 1.7387\n",
      "step 2000: train loss 1.4625, val loss 1.6551\n",
      "step 2500: train loss 1.3975, val loss 1.6061\n",
      "step 3000: train loss 1.3481, val loss 1.5707\n",
      "step 3500: train loss 1.3106, val loss 1.5493\n",
      "step 4000: train loss 1.2786, val loss 1.5267\n",
      "step 4500: train loss 1.2490, val loss 1.5211\n",
      "step 5000: train loss 1.2283, val loss 1.5038\n",
      "step 5500: train loss 1.2082, val loss 1.4982\n",
      "step 6000: train loss 1.1897, val loss 1.4985\n",
      "step 6500: train loss 1.1680, val loss 1.4862\n",
      "step 7000: train loss 1.1521, val loss 1.4851\n",
      "step 7500: train loss 1.1355, val loss 1.4855\n",
      "step 7999: train loss 1.1198, val loss 1.4897\n",
      "\n",
      "Thingsmen, dark, with us: therefore word thou, dead tale.\n",
      "\n",
      "CORIOLANUS:\n",
      "What, pestime to speak--\n",
      "Untilly must cleave, unshap of nine lest,\n",
      "But unfrighties partience how it would for right\n",
      "Make divied. Farewell; we have no sames to woe\n",
      "That contunceies are business\n",
      "Fustily and Thatsmen's virtues.\n",
      "\n",
      "MENENIUS:\n",
      "Why, sir, the most smailock me offer'd.\n",
      "And, you reportion here mischance of the court,\n",
      "Which you known most consorted help him home\n",
      "To make her sleeps too take.\n",
      "\n",
      "BENVOLIO:\n",
      "Tybalt, stir; and th\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            # learning: using sampling over evaluating against entire datasets for efficiency\n",
    "            X, Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out\n",
    "\n",
    "for i in range(max_iters):\n",
    "  xb, yb = get_batch('train', batch_size, block_size)\n",
    "  _, loss = m(xb, yb)\n",
    "  loss.backward()\n",
    "\n",
    "  if i % eval_interval == 0 or i == max_iters-1:\n",
    "    losses = estimate_loss()\n",
    "    print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "  # learning: forgot optimizer.step(). That is used to update the params\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, 500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Results analysis\n",
    "### experiment 1\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 6\n",
    "n_embd = 32\n",
    "n_head = 4\n",
    "n_layer = 3\n",
    "max_iters = 5000\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "\n",
    "22849 parameters\n",
    "\n",
    "Results:\n",
    "- Training time: 25.1s\n",
    "- Performance: train loss 2.1236, val loss 2.1695\n",
    "\n",
    "### experiment 2\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 6\n",
    "n_embd = 32\n",
    "n_head = 4\n",
    "n_layer = 3\n",
    "max_iters = 10000 # increased\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "\n",
    "22849 parameters\n",
    "\n",
    "Results\n",
    "- Training time: 62.5s\n",
    "- Performance: train loss 2.0394, val loss 2.1237\n",
    "\n",
    "### experiment 3\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 32 # increased\n",
    "n_embd = 32\n",
    "n_head = 4\n",
    "n_layer = 3\n",
    "max_iters = 10000\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "\n",
    "23681 parameters\n",
    "\n",
    "Results:\n",
    "- Training time: 2m 36.3s\n",
    "- Performance: train loss 1.8381, val loss 1.9865\n",
    "\n",
    "### experiment 4\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 32\n",
    "n_embd = 32\n",
    "n_head = 4\n",
    "n_layer = 3\n",
    "max_iters = 10000\n",
    "learning_rate = 3e-4 # decreased from 1e-3\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "\n",
    "23681 parameters\n",
    "\n",
    "Results\n",
    "- Training time: 2m 36.3s\n",
    "- Performance: train loss 2.0735, val loss 2.1210\n",
    "\n",
    "Observation: This learning rate is too slow for the majority of training\n",
    "\n",
    "### experiment 5\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 32\n",
    "n_embd = 32\n",
    "n_head = 4\n",
    "n_layer = 6 # increased\n",
    "max_iters = 10000\n",
    "learning_rate = 1e-3\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "\n",
    "42113 parameters\n",
    "\n",
    "Results\n",
    "- Training time: 4m 58s\n",
    "- Performance: train loss 1.7351, val loss 1.8810\n",
    "\n",
    "Observation: The losses got below 2.0 with half of 10000 iterations\n",
    "\n",
    "### experiment 6\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 32\n",
    "n_embd = 128 # increased\n",
    "n_head = 4\n",
    "n_layer = 6\n",
    "max_iters = 10000\n",
    "learning_rate = 1e-3\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "\n",
    "610625 parameters\n",
    "\n",
    "Results\n",
    "- Training time: 8m 4.5s\n",
    "- Performance: train loss 1.4459, val loss 1.6770\n",
    "\n",
    "### experiment 7 (too long to train)\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 256 # increased\n",
    "n_embd = 128\n",
    "n_head = 4\n",
    "n_layer = 6\n",
    "max_iters = 10000\n",
    "learning_rate = 1e-3\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "\n",
    "639297 parameters\n",
    "\n",
    "Results\n",
    "- Training time: too long to train\n",
    "- Performance: too long to train\n",
    "\n",
    "Observation: this only increased the size of `pos_embd_table`. The parameters count didn't increase by much relatively speaking. However, Training takes significantly longer. It took 6 minutes to complete 500 iterations, that is 2hr estimated for 10000 iterations.\n",
    "\n",
    "### experiment 8: see if this is faster than set 7\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 32\n",
    "n_embd = 256 # increased\n",
    "n_head = 4\n",
    "n_layer = 6\n",
    "max_iters = 10000\n",
    "learning_rate = 1e-3\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "\n",
    "2,400,833 parameters (GPT3 has 175b params)\n",
    "\n",
    "Results\n",
    "- Training time: 14m 45s\n",
    "- Performance: train loss 1.4654, val loss 1.6836\n",
    "\n",
    "Observation: \n",
    "1. This is much faster than set 7. 500 iterations took about a minute. Seems like these params aren't created equal when it comes to impacts on training speed. Interestingly, the doubling of `n_embd` increased the parameter count by 4 folds while the increase of `block_size` only added about 30000 params to the count. \n",
    "2. There is no perf improvement over set 6. The second half of 10000 iterations didn't do much in terms of reducing the losses\n",
    "\n",
    "**Question: Why is the training time not proportional to the increase of param count?**\n",
    "AN (Claude): \n",
    "- The large increase in block_size (set 7) significantly slowed down training due to the quadratic complexity of attention mechanisms.\n",
    "- Increasing n_embd (set 8) increased the parameter count more but had a smaller impact on training time. This could be due to a combination of factors including better cache utilization and the M2's ability to handle larger matrix operations relatively efficiently.\n",
    "- the key point is the impact on training time depends more on how the parameters affect the computational structure of the model rather than just the raw number of parameters. \n",
    "\n",
    "### experiment 9: add layernorm to see if the second half of training works better\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 32\n",
    "n_embd = 256\n",
    "n_head = 4\n",
    "n_layer = 6\n",
    "max_iters = 10000\n",
    "learning_rate = 1e-3\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "3. layernorm\n",
    "\n",
    "2,407,489 parameters (GPT3 has 175b params)\n",
    "\n",
    "Results\n",
    "- Training time: 14m\n",
    "- Performance: train loss 1.3485, val loss 1.5908\n",
    "\n",
    "Observations: \n",
    "- I'm not sure if the second half of the training worked better but the training performance improved over experiment 8. Layernorm contributed to the overall training effectiveness.\n",
    "- overfitting is significant given the gap between train loss and val loss\n",
    "\n",
    "### experiment 10: add dropout to reduce overfitting\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 32\n",
    "n_embd = 256\n",
    "n_head = 4\n",
    "n_layer = 6\n",
    "max_iters = 10000\n",
    "learning_rate = 1e-3\n",
    "dropout = 0.2 # added\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "3. layernorm\n",
    "4. dropout\n",
    "\n",
    "2,407,489 parameters (GPT3 has 175b params)\n",
    "\n",
    "Results\n",
    "- Training time: 17m 40s\n",
    "- Performance: train loss 1.4242, val loss 1.6229\n",
    "\n",
    "Observation: dropout seemed to maintain the gap between train loss and val loss. But it also seemed to hamper overall training performance. This run is worse than the previous even though overfitting is less severe.\n",
    "\n",
    "## GPU training\n",
    "\n",
    "### experiment 11: same set up as experiment 10, see how much faster the GPU can make possible\n",
    "GPU instance: \n",
    "gpu_1x_a100_sxm4, Lambda Labs\n",
    "\n",
    "Results\n",
    "- training time: 5m\n",
    "- Performance: train loss 1.4252, val loss 1.6392\n",
    "Observation: 1/3 the time of CPU training.\n",
    "\n",
    "### experiment 12: same set up but use an instance of multiple gpus\n",
    "GPU instance: \n",
    "gpu_1x_a100_sxm4, Lambda Labs\n",
    "\n",
    "Results\n",
    "- training time: 4m\n",
    "- Performance: train loss 1.4305, val loss 1.6353\n",
    "Observation:\n",
    "- no improvement over previous. I probably need to set up compute distribution to utilize the multiple GPUs.\n",
    "- and I did. I attempted to use `nn.DataParallel` for simpler multi-GPU set up but it doesn't work with the custom definition of transformer. DDP is used in https://github.com/karpathy/nanoGPT/blob/master/train.py#L8 and is recommended. But since it is more complex, I will do this when I study the nanoGPT train code.\n",
    "\n",
    "### experiment 13: same set up but use a better GPU\n",
    "GPU instance: \n",
    "gpu_1x_h100_pcie, Lambda Labs\n",
    "\n",
    "Results\n",
    "- training time: 9m 30s\n",
    "- Performance: train loss 1.4297, val loss 1.6355\n",
    "Observation: this is so much slower than gpu_1x_a100_sxm4. Why is that? I guess it is not that important to have an answer to this now\n",
    "\n",
    "### experiment 14: scaling up the model further\n",
    "GPU instance: \n",
    "gpu_1x_a100_sxm4, Lambda Labs\n",
    "\n",
    "```\n",
    "batch_size = 64 # increased\n",
    "block_size = 256 # increased\n",
    "n_embd = 384 # increased\n",
    "n_head = 6 # increased\n",
    "n_layer = 6\n",
    "max_iters = 10000\n",
    "learning_rate = 1e-3\n",
    "dropout = 0.2\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "3. layernorm\n",
    "4. dropout\n",
    "\n",
    "5,466,689 parameters (GPT3 has 175b params)\n",
    "\n",
    "Results\n",
    "- Training time: 16m\n",
    "- Performance: train loss 0.8349, val loss 1.5687\n",
    "observation: this training run massively overfit the model in the second half of the training. let me reduce the learning rate and see if overfitting gets any better\n",
    "\n",
    "```\n",
    "step 0: train loss 4.2937, val loss 4.2903\n",
    "step 500: train loss 1.6452, val loss 1.8075\n",
    "step 1000: train loss 1.3973, val loss 1.6076\n",
    "step 1500: train loss 1.2997, val loss 1.5436\n",
    "step 2000: train loss 1.2340, val loss 1.5108\n",
    "step 2500: train loss 1.1880, val loss 1.4952\n",
    "step 3000: train loss 1.1508, val loss 1.4896\n",
    "step 3500: train loss 1.1157, val loss 1.4844\n",
    "step 4000: train loss 1.0861, val loss 1.4781\n",
    "step 4500: train loss 1.0569, val loss 1.4957\n",
    "step 5000: train loss 1.0328, val loss 1.5006\n",
    "step 5500: train loss 1.0064, val loss 1.5027\n",
    "step 6000: train loss 0.9837, val loss 1.5126\n",
    "step 6500: train loss 0.9603, val loss 1.5160\n",
    "step 7000: train loss 0.9385, val loss 1.5198\n",
    "step 7500: train loss 0.9175, val loss 1.5382\n",
    "step 8000: train loss 0.8999, val loss 1.5411\n",
    "step 8500: train loss 0.8814, val loss 1.5569\n",
    "step 9000: train loss 0.8643, val loss 1.5571\n",
    "step 9500: train loss 0.8486, val loss 1.5714\n",
    "step 9999: train loss 0.8349, val loss 1.5687\n",
    "```\n",
    "\n",
    "### experiment 15: use a smaller learning rate\n",
    "```\n",
    "batch_size = 64 # increased\n",
    "block_size = 256 # increased\n",
    "n_embd = 384 # increased\n",
    "n_head = 6 # increased\n",
    "n_layer = 6\n",
    "max_iters = 8000 # reduced too so it doesn't run for too long\n",
    "learning_rate = 3e-4 # reduced\n",
    "dropout = 0.2\n",
    "```\n",
    "\n",
    "Results\n",
    "- Training time: 10m 40s\n",
    "- Performance: step 7999: train loss 1.1198, val loss 1.4897\n",
    "Observation: 5000 steps are enough. The rest didn't do anything useful"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
