{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Load the data set\n",
    "with open('input.txt', 'r') as f:\n",
    "  input = f.read()\n",
    "chars = sorted(list(set(input)))\n",
    "vocab_size = len(chars)\n",
    "print(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# simple tokenizor: mappings and encode and decode functions\n",
    "stoi = {s: i for i, s in enumerate(chars)}\n",
    "itos = {i: s for i, s in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# encode the entire text dataset\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "data = torch.tensor(encode(input))\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and val sets\n",
    "# learning: this split is simple, the entire encoded set is simply split into two.\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "# data loader\n",
    "\"\"\"\n",
    "learning: notice the difference between the organization of this data and that of makemore (i.e., name) data.\n",
    "The makemore data is simpler in that it consists of names, which structures the data with each name having an start and an end. \n",
    "A block, defined by the block_size, operates within a name. On the contrary, this shakespera data is a blob of \n",
    "text, there is no inherent structure to it and hence allows abitrary slicing. A block in this context operates \n",
    "in the entire text blob and the positioning of it is determined by a random process (i.e., randint()) since there is \n",
    "no natural start points.\n",
    "WAIT, the above analysis doesn't seem to be complete. Even though there is a natural start and end point of a name, \n",
    "the `def build_dataset(words):` function uses a block to slide through a word and repeats for all words. And the \n",
    "X in data contains examples of blocks instead of words. And then, I can use the same technique (i.e., block sliding)\n",
    "for the entire text blob in this data, producing a dataset of a size of (len(text) - block_size).\n",
    "So I guess they are different options for setting up the data.\n",
    "\n",
    "An input block is reused (see `when input is... the target: ...` logs) to max training opportunities.\n",
    "\n",
    "learning: torch.stack() turns\n",
    "[tensor([24, 43, 58,  5, 57,  1, 46, 43]),\n",
    "  tensor([44, 53, 56,  1, 58, 46, 39, 58]),\n",
    "  tensor([52, 58,  1, 58, 46, 39, 58,  1]),\n",
    "  tensor([25, 17, 27, 10,  0, 21,  1, 54])]\n",
    "into\n",
    "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
    "         [44, 53, 56,  1, 58, 46, 39, 58],\n",
    "         [52, 58,  1, 58, 46, 39, 58,  1],\n",
    "         [25, 17, 27, 10,  0, 21,  1, 54]])\n",
    "\"\"\"\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "  data = train_data if split == 'train' else val_data\n",
    "  idx = torch.randint(len(data)-block_size, (batch_size,))\n",
    "  # learning: I didn't use torch.stack\n",
    "  x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "  return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "# learning: the targets in yb are index into the next char\n",
    "print('targets')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "# note: this structure in the training data isn't needed for the bigram model training\n",
    "# note: this structure in the training data is only applied to the attention layers during the training of transformer model\n",
    "for b in range(batch_size):\n",
    "  for t in range(block_size):\n",
    "    # learning: I forgot the indexing of time dimension is `:t+1`\n",
    "    context = xb[b, :t+1]\n",
    "    target = yb[b, t]\n",
    "    print(f'when input is {context.tolist()} the target: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.7319, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "wftE:fU\n",
      "-&p?L3HdvPQSQ.FNFW:OqzUC:llUpuNY3p&HBho3pRjU\n",
      "iwSsMMRKvqYedh.3p;EdWxcAifQeEqXU'JVNYPWjc\n",
      "z,-iN\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    # learning: this token_embedding_table is bound to a nn.Embedding object\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    # TODO: I need a positional embedding table\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    # learning: I didn't know this guy returns logits\n",
    "    # learning: self.token_embedding_table[idx] and got TypeError: 'Embedding' object is not subscriptable\n",
    "    # I forgot that nn.Embedding is a class and it takes inputs as described in https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "    logits = self.token_embedding_table(idx)\n",
    "\n",
    "    # got RuntimeError: Expected target size [4, 65], got [4, 8] when `loss = F.cross_entropy(logits, targets)`\n",
    "    # to fix that, we transform them into 2d tensors by merging the B and T dimensions using `.view`\n",
    "    if targets is not None:\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B*T, C)  # B*T shows T is useless outside of the attention module\n",
    "      targets = targets.view(B*T)\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "    else:\n",
    "      loss = None\n",
    "    \n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # learning: I need to start with the form of generation. In this case, the form is that of ChatGPT\n",
    "    # I provide a prompt (i.e., idx) and the model use that to generate texts of length max_new_tokens\n",
    "    for _ in range(max_new_tokens):\n",
    "      logits, _ = self(idx)\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      idx_next = torch.multinomial(probs, num_samples=1)\n",
    "      # learning: idx.append(idx_next) -> AttributeError: 'Tensor' object has no attribute 'append'\n",
    "      idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "# learning: the out.shape is [4,8,65] because we haven't done multinomial on the 65 logits yet. Therefore,\n",
    "# we have the last dimension of 65\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "# Use `torch.zeros((1,1), dtype=torch.long)` as the prompt. `torch.long` is necessary because the default \n",
    "# dtype is float with `zeros`\n",
    "print(m.generate(torch.zeros((1,1), dtype=torch.long), 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A jS,\n",
      "fr'k tcaant ngandechoutonicomyby n, dis Gorllllin dr, t ke zhowa te the 'Then perar Ay hw\n",
      "CO:\n",
      "CHChygurs-\n",
      "S r a t lk:\n",
      "Q olta cu\n",
      "Ca, s ARDelids GS fe:\n",
      "YY&cavefed re; sche,d ss! IZqSDXve pr s'Coyoomo?\n",
      "CIqAll: u:\n",
      "\n",
      "Wh mb, wesapiQero' giver.\n",
      "I br th g JONyom whem tro ceriese s it purfr,t d,'mef cir l itheay\n",
      "and'OKfand 'e wis toshurkichis'l a ss Binor,' d ngh or t s fondZTESMinccan,\n",
      "The she notomy he pe f d ave wnING hecby dr h me u.\n",
      "MNORINous ngof ayo may;d th Abe l lou pthepatur,\n",
      "Ane? pouthit c\n"
     ]
    }
   ],
   "source": [
    "# train the bigram model with torch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters())\n",
    "\n",
    "for steps in range(10000):\n",
    "  xb, yb = get_batch('train')\n",
    "  logits, loss = m(xb, yb)\n",
    "\n",
    "  loss.backward()\n",
    "  # learning: I put zero_grad() before step() and no learning occurred. `.data -= lr * .grad`, nothing happens\n",
    "  # when `.grad` is 0\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "print(m.generate(torch.zeros((1,1), dtype=torch.long), 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3794,  0.2624],\n",
      "        [-1.2309,  0.4991],\n",
      "        [-0.3531, -0.6193],\n",
      "        [-0.8199, -1.8553],\n",
      "        [-1.2193,  0.2188],\n",
      "        [-1.8425,  0.2470],\n",
      "        [ 1.3761, -0.6989],\n",
      "        [ 0.8018,  0.1684]])\n",
      "tensor([[-1.3794,  0.2624],\n",
      "        [-1.3051,  0.3808],\n",
      "        [-0.9878,  0.0474],\n",
      "        [-0.9458, -0.4282],\n",
      "        [-1.0005, -0.2988],\n",
      "        [-1.1408, -0.2079],\n",
      "        [-0.7813, -0.2780],\n",
      "        [-0.5834, -0.2222]])\n"
     ]
    }
   ],
   "source": [
    "# aggregate past context. The input and output share the same shape: (B, T, C)\n",
    "# note: there is a nested loop approach and a softmax approach to this\n",
    "# B,T,C = 4,8,2\n",
    "# x = torch.randn(B,T,C)\n",
    "# xbow = torch.zeros((B,T,C))\n",
    "# wei = torch.tril(torch.ones((T,T)))\n",
    "# aver = wei / torch.sum(wei, 1, keepdim=True)\n",
    "# xbow = aver @ x\n",
    "# print(x[0])\n",
    "# print(xbow[0])\n",
    "\n",
    "# the softmax approach\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # this wei is 2d\n",
    "aver = torch.softmax(wei, dim=-1)  # this is the \"attention pattern\" described in 1B3B video\n",
    "xbow = aver @ x # (T T) @ (T 2)\n",
    "print(x[0])\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern: \n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7843, 0.2157, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2240, 0.3344, 0.4416, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0302, 0.6503, 0.1961, 0.1235, 0.0000, 0.0000],\n",
      "        [0.1168, 0.7052, 0.0851, 0.0739, 0.0189, 0.0000],\n",
      "        [0.1463, 0.2580, 0.1649, 0.1042, 0.1172, 0.2093]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "value matrix before weighted sum (i.e., wei @ v):\n",
      "tensor([[-0.7059,  0.4846,  0.5991, -0.2759,  0.0818,  0.0714, -0.0364,  0.1166],\n",
      "        [-0.0129, -0.5899,  0.6649,  0.6830, -1.0855,  0.3631,  0.4322, -0.4728],\n",
      "        [ 0.4290, -0.2209,  0.6159,  0.1923,  0.6443, -0.4568,  0.5005, -0.0281],\n",
      "        [-0.6370, -1.1575, -0.3756,  0.4416, -0.6450, -0.0261,  0.0154,  1.1953],\n",
      "        [ 1.1061,  0.5586, -0.1329, -0.5069,  0.5895,  0.0889, -0.0968, -0.3687],\n",
      "        [-0.2447, -1.0041, -0.5983,  0.1493, -0.6762, -0.6977,  0.9403,  1.0576]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "output of the self attention head: \n",
      "tensor([[-0.7059,  0.4846,  0.5991, -0.2759,  0.0818,  0.0714, -0.0364,  0.1166],\n",
      "        [-0.5564,  0.2528,  0.6133, -0.0690, -0.1700,  0.1343,  0.0647, -0.0106],\n",
      "        [ 0.0270, -0.1863,  0.6285,  0.2515, -0.0601, -0.0643,  0.3574, -0.1444],\n",
      "        [-0.0242, -0.5552,  0.5248,  0.5281, -0.6567,  0.1455,  0.3799, -0.1618],\n",
      "        [-0.0812, -0.4532,  0.5610,  0.4889, -0.7377,  0.2253,  0.3424, -0.2408],\n",
      "        [-0.0238, -0.3830,  0.1809,  0.1854, -0.3015, -0.1095,  0.3758,  0.1931]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# turn the dumb aggregate method into a single head of self attention\n",
    "B,T,C = 4,6,8\n",
    "head_size = 8\n",
    "x = torch.randn(B,T,C)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "q = query(x) # B, T, head_size\n",
    "k = key(x) # B, T, head_size\n",
    "v = value(x) # B, T, head_size\n",
    "wei = q @ k.transpose(-2, -1)  # this wei is 3d. that is why softmax's dim can't be 1, but is -1\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = torch.softmax(wei, dim=-1) # B, T, T\n",
    "print('attention pattern: ')\n",
    "print(wei[0])\n",
    "print('value matrix before weighted sum (i.e., wei @ v):')\n",
    "print(v[0])\n",
    "out = wei @ v  # B, T, head_size\n",
    "print('output of the self attention head: ')\n",
    "print(out[0])  # I can see how the weighted sum affects the output of the attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# adding a linear layer to the bigram model\n",
    "# adding a positional embedding to the model\n",
    "pos_embedding_table = nn.Embedding(8, 32)\n",
    "pos_embedding_table(torch.arange(8))\n",
    "print(torch.allclose(pos_embedding_table.weight, pos_embedding_table(torch.arange(8))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.1128e-01,  7.1639e-01, -6.5479e-01,  1.0583e-01, -8.2697e-02,\n",
       "           5.3774e-01, -1.0094e+00, -8.5511e-01, -2.8455e-01, -4.4945e-02,\n",
       "           5.6408e-01,  4.6272e-01,  5.2987e-01, -1.2838e+00,  1.9325e-01,\n",
       "           1.0372e-01,  1.3753e+00, -1.1843e+00, -3.5781e-01,  6.6724e-01,\n",
       "          -3.3983e-01,  4.4522e-01, -5.3159e-01,  7.8568e-01,  1.3475e-01,\n",
       "          -7.8972e-02,  5.0798e-01,  9.0163e-01,  1.0944e+00, -5.9593e-01,\n",
       "          -4.7291e-01, -9.6337e-02],\n",
       "         [ 3.2456e-01,  4.3658e-01, -6.0482e-01,  3.3561e-02,  2.2846e-01,\n",
       "           1.0905e+00, -6.0211e-01, -4.3757e-01, -1.3601e-01,  4.0826e-02,\n",
       "           8.7570e-01,  1.0827e-01,  3.8885e-01, -7.7548e-01,  4.3872e-01,\n",
       "          -3.7804e-01,  2.6918e-01, -1.4338e+00, -1.5597e+00, -1.3550e-01,\n",
       "           4.3272e-01, -2.7150e-01, -2.5645e-02,  6.5949e-02,  1.4966e-01,\n",
       "          -3.2941e-01,  6.5958e-01,  3.3163e-01,  8.2996e-01, -7.9754e-01,\n",
       "          -3.7881e-01, -2.2242e-01],\n",
       "         [ 6.7855e-01,  5.4062e-01, -2.1162e-01,  2.2810e-01,  1.9517e-01,\n",
       "           5.4186e-01,  1.3171e-01, -2.3979e-01, -1.0741e-01,  3.1626e-01,\n",
       "           5.6150e-01,  7.3937e-02,  1.2131e-01, -2.8499e-01,  6.0286e-01,\n",
       "          -4.4250e-01,  1.9312e-01, -9.9543e-01, -1.2971e+00, -6.5306e-02,\n",
       "           3.2290e-01, -4.0487e-01, -1.5704e-01,  1.6244e-01,  2.3247e-02,\n",
       "          -4.4478e-01,  7.5088e-01,  1.1234e-01,  6.0676e-01, -6.9662e-01,\n",
       "          -2.4155e-01, -4.3020e-01],\n",
       "         [ 5.4843e-01,  2.9726e-01, -3.6254e-01,  6.1039e-02, -1.2938e-01,\n",
       "           7.1528e-01,  1.0931e-02, -1.7259e-02,  1.4545e-01, -4.1078e-02,\n",
       "           6.2035e-01,  1.8298e-02,  4.4366e-01, -5.2247e-01,  8.9133e-01,\n",
       "          -5.7604e-01, -1.1337e-02, -7.5919e-01, -9.9771e-01,  2.2701e-01,\n",
       "           7.9016e-02, -2.5332e-01, -5.9125e-02,  2.4980e-01,  6.1626e-02,\n",
       "          -3.1272e-01,  2.4583e-01, -6.4313e-02,  2.1200e-01, -6.3776e-01,\n",
       "          -2.2928e-01, -4.1832e-01],\n",
       "         [ 4.2761e-01,  2.3748e-01, -3.8591e-01,  5.5619e-02, -1.3596e-01,\n",
       "           5.0074e-01, -6.2710e-02,  4.2644e-02, -1.0247e-02,  1.6478e-01,\n",
       "           3.7103e-01,  3.0164e-03,  3.0789e-01, -2.6665e-01,  6.6874e-01,\n",
       "          -5.3163e-01, -2.7849e-02, -4.9475e-01, -9.4688e-01,  2.0260e-01,\n",
       "           1.7252e-01, -3.7259e-01, -5.8978e-02,  2.4494e-01, -4.4601e-02,\n",
       "          -2.2368e-01,  4.0137e-01, -8.8112e-03,  6.6742e-02, -4.6037e-01,\n",
       "          -1.1935e-01, -3.5545e-01],\n",
       "         [ 5.2025e-01,  1.2061e-01, -1.6716e-01,  2.7499e-01, -2.1675e-01,\n",
       "           3.7196e-01,  1.7710e-01,  1.2199e-01, -9.1968e-02,  2.3003e-01,\n",
       "           2.4884e-01,  8.1458e-02,  5.2573e-02,  8.4554e-02,  5.3276e-01,\n",
       "          -4.1910e-01,  1.0458e-01, -4.7472e-01, -8.1763e-01,  1.0711e-01,\n",
       "           2.0585e-01, -2.2569e-01, -8.5749e-02,  1.8902e-01, -2.1882e-01,\n",
       "          -2.7411e-01,  6.5327e-01,  1.6580e-01, -2.4406e-02, -2.1679e-01,\n",
       "          -1.1187e-01, -7.5040e-02]],\n",
       "\n",
       "        [[-2.1144e-01,  5.6968e-01, -4.5227e-01, -6.7791e-01,  8.3359e-01,\n",
       "           8.5024e-01,  2.2297e-03, -9.2884e-01, -6.7254e-01, -4.8703e-01,\n",
       "           3.2296e-01, -7.8583e-01, -7.2328e-01, -2.2421e-01, -4.4875e-02,\n",
       "           4.0175e-01,  2.0295e-02, -9.3783e-01,  1.5547e-01,  9.5927e-01,\n",
       "          -2.5044e-01,  1.1453e-01,  7.0871e-01,  8.2961e-02,  3.3837e-01,\n",
       "          -2.4898e-01, -3.0871e-01,  5.0219e-01,  3.9310e-01, -1.2215e+00,\n",
       "          -7.4223e-02,  3.1569e-01],\n",
       "         [ 1.8633e-01,  3.2862e-01, -1.7455e-01, -5.2124e-01,  2.7288e-01,\n",
       "           4.4284e-01,  9.3458e-02, -3.8093e-01, -4.8037e-01, -1.0584e-01,\n",
       "          -5.4843e-02, -1.5871e-01, -3.8696e-01, -1.9514e-01,  1.0047e-01,\n",
       "           5.7095e-01, -1.6568e-02, -2.5944e-01,  2.4983e-03,  4.9969e-01,\n",
       "          -2.8928e-01,  7.9130e-03,  1.0633e-01,  2.1454e-01,  1.2287e-01,\n",
       "          -3.1577e-01, -3.6842e-01, -8.8482e-02,  6.0888e-01, -1.9569e-01,\n",
       "           1.1991e-01, -2.6661e-01],\n",
       "         [-3.1712e-02,  1.5503e-01, -1.7539e-01, -2.2753e-01,  1.1195e-01,\n",
       "           2.1793e-01,  9.9058e-02, -3.7023e-01, -5.6021e-01,  1.4957e-02,\n",
       "           2.1393e-02, -1.6256e-01, -3.5252e-01,  5.7352e-02,  1.0851e-01,\n",
       "           5.4399e-01,  4.5748e-02, -3.9162e-01,  2.6966e-01,  2.6440e-01,\n",
       "          -1.3371e-02, -7.4094e-02,  3.5962e-01,  1.1105e-01,  1.7979e-01,\n",
       "          -3.4394e-01, -1.5297e-01,  2.8164e-01,  4.7697e-01, -3.3492e-01,\n",
       "          -1.4647e-01,  1.9252e-01],\n",
       "         [-1.6083e-01,  5.8326e-02, -3.5014e-01, -2.9838e-01,  3.9667e-02,\n",
       "           3.8238e-02,  1.3559e-01, -4.3575e-01, -5.1476e-01,  1.0098e-01,\n",
       "           1.8581e-01,  6.4951e-02, -3.2376e-01,  1.3107e-01,  2.2873e-02,\n",
       "           4.2465e-01,  2.4317e-01, -3.6650e-01,  2.7146e-01,  1.2402e-01,\n",
       "           1.0342e-02, -1.1924e-01,  2.4984e-01, -1.1356e-01,  2.1743e-01,\n",
       "          -1.7858e-01, -1.8049e-02,  4.0359e-01,  3.4801e-01, -4.5738e-01,\n",
       "          -3.0976e-01,  4.3637e-01],\n",
       "         [-6.9033e-02,  5.9730e-02, -1.8677e-01, -1.9035e-01,  2.9363e-02,\n",
       "          -3.4235e-03,  1.5243e-01, -3.3146e-01, -2.9176e-01,  1.6004e-01,\n",
       "           1.1917e-02,  1.2415e-01, -2.2179e-01,  1.8358e-01, -1.1962e-02,\n",
       "           3.9690e-01,  1.4100e-01, -2.2920e-01,  3.7136e-01,  1.2937e-01,\n",
       "           5.8940e-02, -1.5915e-01,  3.4911e-01, -8.3795e-02,  3.4909e-01,\n",
       "          -4.2504e-02, -6.0683e-02,  1.1528e-01,  4.5432e-01, -1.1749e-01,\n",
       "          -2.3926e-01,  3.4186e-01],\n",
       "         [-1.4811e-01, -7.1396e-02, -1.7858e-01, -2.5847e-01,  6.6380e-02,\n",
       "           3.2505e-02,  9.8630e-02, -4.2925e-01, -3.9335e-01,  2.1618e-01,\n",
       "           5.3510e-02, -1.8515e-02, -3.1370e-01,  2.0484e-01,  7.6227e-02,\n",
       "           3.1908e-01,  2.5844e-01, -3.6217e-01,  2.3795e-01,  4.9911e-02,\n",
       "          -2.5794e-02, -1.5285e-01,  3.3515e-01, -1.9534e-01,  3.8340e-01,\n",
       "           2.0958e-02, -7.5437e-02,  1.8031e-01,  5.3088e-01, -1.3924e-01,\n",
       "          -1.4045e-01,  3.8996e-01]],\n",
       "\n",
       "        [[-3.9041e-01, -1.3705e+00, -4.3117e-01, -3.1223e-01,  1.5195e-01,\n",
       "          -3.3205e-01, -2.8266e-01,  8.6877e-01, -3.0410e-01,  8.6237e-01,\n",
       "           1.0254e+00, -1.4506e+00, -1.9982e+00,  3.3827e-02,  7.4338e-01,\n",
       "          -6.0478e-01, -5.1166e-01, -3.2568e-01, -8.2854e-01, -4.9175e-01,\n",
       "          -6.6789e-01, -5.3254e-01,  1.7488e-01, -7.8597e-01, -1.6131e+00,\n",
       "           3.9195e-01, -6.6976e-01,  7.3517e-01, -7.4129e-01,  1.0114e-01,\n",
       "           3.1893e-01,  6.4318e-01],\n",
       "         [ 2.0208e-02, -2.8157e-01, -3.6954e-01, -2.3322e-01,  3.8312e-01,\n",
       "          -3.3859e-01, -4.3184e-01,  6.0419e-01,  1.1838e-02,  7.4777e-01,\n",
       "           7.8273e-01, -5.3018e-01, -1.3928e+00, -2.1219e-01,  6.0036e-01,\n",
       "          -3.1671e-01, -2.7328e-01, -2.6836e-01, -7.0729e-01, -1.3667e-01,\n",
       "          -3.9616e-01, -2.1707e-01,  6.0892e-01, -7.7688e-01, -1.0398e+00,\n",
       "          -1.3565e-01, -5.8251e-01,  3.7507e-01, -5.2898e-01,  1.8314e-01,\n",
       "          -3.0764e-01,  5.5732e-01],\n",
       "         [-1.1669e-02, -9.9634e-02, -4.0731e-01, -2.1027e-01, -7.9578e-03,\n",
       "          -7.0060e-02, -3.5838e-01,  2.0770e-01, -4.0425e-01,  3.1421e-01,\n",
       "           5.2308e-01, -5.6821e-01, -7.0927e-01,  2.5563e-02,  1.3336e-01,\n",
       "          -8.7290e-01,  4.3039e-01, -1.6053e-01, -5.5253e-01,  2.5931e-01,\n",
       "          -1.9872e-01,  1.2883e-01,  2.1994e-01, -4.4539e-01, -6.3243e-01,\n",
       "           2.1543e-01, -2.5382e-01,  2.1301e-01, -4.9382e-01, -9.0950e-02,\n",
       "          -2.4877e-01,  4.3074e-01],\n",
       "         [ 2.1563e-02, -1.8290e-01, -3.7695e-01, -1.2353e-01, -2.5220e-02,\n",
       "          -1.4663e-01, -2.5803e-01,  2.2952e-01, -2.6601e-01,  2.4902e-01,\n",
       "           8.5111e-02, -3.2160e-01, -4.3168e-01,  3.1786e-01,  9.6480e-02,\n",
       "          -7.6440e-01,  2.8402e-01,  6.5000e-02, -4.8707e-01,  2.0854e-01,\n",
       "          -2.2271e-01, -4.7829e-02,  1.0329e-01, -3.1627e-01, -2.3980e-01,\n",
       "           3.0815e-01, -3.2378e-01,  1.2604e-01, -5.2816e-01,  5.0393e-02,\n",
       "          -2.1666e-01,  2.4948e-01],\n",
       "         [ 1.7878e-01, -2.0183e-01, -2.7870e-01, -1.7919e-01, -2.1597e-01,\n",
       "          -1.0616e-01, -2.9731e-01,  2.0264e-01, -2.6380e-01,  2.7204e-01,\n",
       "          -6.6263e-02, -3.6922e-01, -1.8914e-01,  1.8574e-01,  1.0479e-01,\n",
       "          -8.7086e-01, -1.1383e-01,  1.4918e-01, -5.8507e-01, -7.6791e-04,\n",
       "          -2.8914e-01,  1.2139e-01,  6.8911e-02, -3.3871e-01, -1.3869e-01,\n",
       "           9.3678e-02, -3.2069e-01, -1.0035e-01, -3.9485e-01,  8.0625e-02,\n",
       "          -2.7851e-01,  2.3203e-01],\n",
       "         [ 1.6754e-01, -1.7951e-01, -1.5819e-01, -2.6946e-01, -2.4958e-01,\n",
       "          -2.0239e-01, -2.0288e-01,  3.4247e-01, -1.2790e-01,  2.3568e-01,\n",
       "          -7.8032e-03, -3.8086e-01, -6.6069e-02, -8.3657e-02,  1.2317e-01,\n",
       "          -7.0733e-01,  2.1967e-02,  5.1931e-02, -3.2744e-01,  3.0530e-01,\n",
       "          -3.0502e-01,  1.1170e-01,  4.9932e-02, -8.5254e-02, -7.0818e-02,\n",
       "           9.0866e-02, -5.7906e-01, -1.4765e-01, -3.2485e-01,  6.3111e-02,\n",
       "          -3.2455e-01,  1.2399e-01]],\n",
       "\n",
       "        [[-5.0729e-01, -1.4895e-01,  1.5809e+00, -2.3993e-01, -2.0350e-01,\n",
       "           3.4668e-01, -5.3517e-01, -2.5497e-01, -4.3504e-01, -5.3831e-01,\n",
       "           4.9830e-01,  1.8690e-01, -1.0760e+00,  8.2225e-01, -2.9176e-01,\n",
       "           4.9004e-02,  8.5993e-02, -7.1936e-01,  1.4679e-01, -3.4044e-01,\n",
       "           4.0590e-02, -6.8505e-02,  6.5455e-01, -1.2391e+00, -8.6418e-01,\n",
       "           2.7974e-01, -9.3396e-01,  4.2732e-01, -6.6517e-01, -1.1454e-01,\n",
       "          -2.2457e-01,  3.3166e-01],\n",
       "         [ 3.3005e-01,  1.6263e-01,  8.6443e-01,  1.8567e-01, -5.0696e-02,\n",
       "          -7.9888e-02, -5.6098e-02, -5.4221e-03, -2.5763e-02, -2.3443e-01,\n",
       "          -1.7023e-01,  2.8265e-01, -9.5778e-01,  7.3047e-01,  2.2691e-01,\n",
       "           2.0261e-02, -2.1825e-01, -8.6943e-02, -1.1474e-02, -4.3389e-01,\n",
       "           2.4438e-01, -1.4227e-01,  5.2346e-01, -9.0776e-01, -4.0382e-01,\n",
       "          -9.7134e-02, -2.6696e-01,  9.5597e-02, -4.1280e-01, -1.2546e-01,\n",
       "           1.1187e-01,  7.5213e-02],\n",
       "         [ 2.0175e-01,  2.8642e-02,  3.4214e-01,  3.9052e-01, -2.2024e-01,\n",
       "          -2.7933e-01,  5.9930e-02, -2.2541e-02, -2.2642e-01, -1.5499e-01,\n",
       "          -2.5751e-01, -4.4473e-02, -5.5025e-01,  1.6776e-01, -5.4710e-02,\n",
       "           1.3250e-01, -1.0569e-01,  6.0011e-02, -4.9804e-01, -4.0745e-01,\n",
       "           4.5640e-01,  7.7314e-02, -4.6257e-01, -1.9938e-01, -4.7464e-01,\n",
       "          -9.0308e-02, -1.3506e-01,  7.9509e-02, -1.4951e-01, -6.2019e-02,\n",
       "           9.6914e-02,  6.7987e-02],\n",
       "         [-3.5737e-02,  1.6313e-01,  2.1734e-01,  3.2563e-01,  8.4267e-02,\n",
       "          -2.1249e-01, -9.4821e-02, -7.3760e-02,  1.4826e-01,  2.7782e-02,\n",
       "          -2.2702e-01, -6.8619e-02, -2.9407e-01,  1.8838e-01,  3.0063e-02,\n",
       "           2.3447e-02, -2.5936e-01,  6.2349e-02, -2.8662e-01, -1.5649e-01,\n",
       "           4.7384e-01, -1.7056e-01, -2.8101e-02, -3.1240e-01, -3.1346e-01,\n",
       "           2.7613e-02,  7.3189e-02,  8.8284e-02, -1.9673e-02, -2.0447e-01,\n",
       "           3.0993e-01, -2.5277e-01],\n",
       "         [-2.2930e-02,  3.2882e-01, -2.3034e-01,  1.7681e-01,  2.1170e-01,\n",
       "          -3.1138e-02, -2.4275e-01, -6.1508e-02,  1.6837e-01, -2.1126e-01,\n",
       "          -3.0104e-01,  1.2201e-02, -2.4764e-01, -3.2608e-01, -1.9815e-01,\n",
       "          -5.3523e-02, -1.1630e-01, -2.7504e-02, -4.9186e-01,  2.3619e-01,\n",
       "           3.9029e-01,  9.6302e-02, -5.4084e-01,  6.2628e-03, -1.3199e-01,\n",
       "           1.5121e-01, -2.7457e-02, -2.7796e-01,  1.3134e-01, -3.4940e-01,\n",
       "           1.6063e-01, -2.6748e-01],\n",
       "         [ 1.6068e-01,  2.5283e-01, -2.2945e-01,  1.2518e-01,  9.3984e-02,\n",
       "          -8.5205e-02, -2.0483e-01,  5.0809e-02,  1.5057e-01, -1.6710e-01,\n",
       "          -3.2048e-01,  1.0151e-02, -1.8041e-01, -2.7762e-01, -1.6220e-02,\n",
       "          -8.7972e-03, -2.4418e-02, -2.2683e-01, -3.1924e-01,  2.7553e-01,\n",
       "           1.1530e-01,  1.0312e-01, -2.5077e-01, -1.6196e-01, -1.1417e-01,\n",
       "           8.6092e-02, -1.0702e-01, -2.4229e-01,  6.7771e-02, -2.1743e-01,\n",
       "           2.3732e-02, -2.2176e-01]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: complete Block class - DONE\n",
    "# TODO: add residual connections\n",
    "  # TODO: add projections\n",
    "# TODO: add dropouts\n",
    "# TODO: add layernorm\n",
    "\n",
    "B,T,C = 4,6,32\n",
    "\n",
    "class Head(nn.Module):\n",
    "  \"\"\" One head of self attention \"\"\"\n",
    "  def __init__(self, head_size):\n",
    "    super().__init__()\n",
    "    self.key = nn.Linear(C, head_size, bias=False)\n",
    "    self.query = nn.Linear(C, head_size, bias=False)\n",
    "    self.value = nn.Linear(C, head_size, bias=False)\n",
    "    # learning: I forgot to use register_buffer. I use it because I don't want it to be in the computational graph\n",
    "    self.register_buffer('tril', torch.tril(torch.ones((T, T))))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # learning: what I wrote vs. what is enough: B, T, C = x.shape[0], x.shape[1], x.shape[2]\n",
    "    q = self.query(x)\n",
    "    k = self.key(x)\n",
    "    v = self.value(x)\n",
    "    # learning: the normalization base is C, I used 2\n",
    "    wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "    wei = wei.masked_fill(self.tril == 0, float('-inf'))\n",
    "    wei = torch.softmax(wei, dim=-1)\n",
    "    out = wei @ v\n",
    "    return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  \"\"\" Multiple heads of self attention in parallel \"\"\"\n",
    "  def __init__(self, n_head):\n",
    "    super().__init__()\n",
    "    head_size = C // n_head\n",
    "    # self.h = Head(head_size)\n",
    "    # learning: the above was what I did. I should instead put these Heads in a ModuleList\n",
    "    self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
    "    self.proj = nn.Linear(head_size * n_head, C, bias=False)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # return torch.cat((self.h(x), self.h(x), self.h(x), self.h(x)), dim=-1)\n",
    "    # learning: the above was what I did. I should instead put these Heads in a ModuleList\n",
    "    # Because this class is a representation of MULTI heads instead of a single head being forwarded multiple times\n",
    "    x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    x = self.proj(x)\n",
    "    return x\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "  \"\"\" This layer is applied to each position in a block locally and independently \"\"\"\n",
    "  def __init__(self, n_embd):\n",
    "    super().__init__()\n",
    "    # learning: I forgot the non-linearity. With the non-linearity, I also need nn.Sequential\n",
    "    self.net = nn.Sequential(\n",
    "      nn.Linear(n_embd, n_embd, bias=False),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(n_embd, n_embd, bias=False)  # projection layer\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.net(x)\n",
    "  \n",
    "class Block(nn.Module):\n",
    "  \"\"\" Transformer block: Communicate-then-compute\"\"\"\n",
    "\n",
    "  def __init__(self, n_head):\n",
    "    super().__init__()\n",
    "    self.sa = MultiHeadAttention(n_head=n_head)\n",
    "    self.ffwd = FeedFoward(n_embd=C)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = x + self.sa(x)\n",
    "    x = x + self.ffwd(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT definition using the layers defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# ...\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# TODO: use CPU to train\n",
    "# TODO: use GPU to train\n",
    "# TODO: review nanoGPT training script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
