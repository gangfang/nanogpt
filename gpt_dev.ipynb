{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Load the data set\n",
    "with open('input.txt', 'r') as f:\n",
    "  input = f.read()\n",
    "chars = sorted(list(set(input)))\n",
    "vocab_size = len(chars)\n",
    "print(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# simple tokenizor: mappings and encode and decode functions\n",
    "stoi = {s: i for i, s in enumerate(chars)}\n",
    "itos = {i: s for i, s in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# encode the entire text dataset\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "data = torch.tensor(encode(input))\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and val sets\n",
    "# learning: this split is simple, the entire encoded set is simply split into two.\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "# data loader\n",
    "\"\"\"\n",
    "learning: notice the difference between the organization of this data and that of makemore (i.e., name) data.\n",
    "The makemore data is simpler in that it consists of names, which structures the data with each name having an start and an end. \n",
    "A block, defined by the block_size, operates within a name. On the contrary, this shakespera data is a blob of \n",
    "text, there is no inherent structure to it and hence allows abitrary slicing. A block in this context operates \n",
    "in the entire text blob and the positioning of it is determined by a random process (i.e., randint()) since there is \n",
    "no natural start points.\n",
    "WAIT, the above analysis doesn't seem to be complete. Even though there is a natural start and end point of a name, \n",
    "the `def build_dataset(words):` function uses a block to slide through a word and repeats for all words. And the \n",
    "X in data contains examples of blocks instead of words. And then, I can use the same technique (i.e., block sliding)\n",
    "for the entire text blob in this data, producing a dataset of a size of (len(text) - block_size).\n",
    "So I guess they are different options for setting up the data.\n",
    "\n",
    "An input block is reused (see `when input is... the target: ...` logs) to max training opportunities.\n",
    "\n",
    "learning: torch.stack() turns\n",
    "[tensor([24, 43, 58,  5, 57,  1, 46, 43]),\n",
    "  tensor([44, 53, 56,  1, 58, 46, 39, 58]),\n",
    "  tensor([52, 58,  1, 58, 46, 39, 58,  1]),\n",
    "  tensor([25, 17, 27, 10,  0, 21,  1, 54])]\n",
    "into\n",
    "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
    "         [44, 53, 56,  1, 58, 46, 39, 58],\n",
    "         [52, 58,  1, 58, 46, 39, 58,  1],\n",
    "         [25, 17, 27, 10,  0, 21,  1, 54]])\n",
    "\"\"\"\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split, batch_size, block_size):\n",
    "  data = train_data if split == 'train' else val_data\n",
    "  idx = torch.randint(len(data)-block_size, (batch_size,))\n",
    "  # learning: I didn't use torch.stack\n",
    "  x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "  return x, y\n",
    "\n",
    "xb, yb = get_batch('train', batch_size, block_size)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "# learning: the targets in yb are index into the next char\n",
    "print('targets')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "# note: this structure in the training data isn't needed for the bigram model training\n",
    "# note: this structure in the training data is only applied to the attention layers during the training of transformer model\n",
    "for b in range(batch_size):\n",
    "  for t in range(block_size):\n",
    "    # learning: I forgot the indexing of time dimension is `:t+1`\n",
    "    context = xb[b, :t+1]\n",
    "    target = yb[b, t]\n",
    "    print(f'when input is {context.tolist()} the target: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(5.0364, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "lfJeukRuaRJKXAYtXzfJ:HEPiu--sDioi;ILCo3pHNTmDwJsfheKRxZCFs\n",
      "lZJ XQc?:s:HEzEnXalEPklcPU cL'DpdLCafBheH\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    # learning: this token_embedding_table is bound to a nn.Embedding object\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    # TODO: I need a positional embedding table\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    # learning: I didn't know this guy returns logits\n",
    "    # learning: self.token_embedding_table[idx] and got TypeError: 'Embedding' object is not subscriptable\n",
    "    # I forgot that nn.Embedding is a class and it takes inputs as described in https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "    logits = self.token_embedding_table(idx)\n",
    "\n",
    "    # got RuntimeError: Expected target size [4, 65], got [4, 8] when `loss = F.cross_entropy(logits, targets)`\n",
    "    # to fix that, we transform them into 2d tensors by merging the B and T dimensions using `.view`\n",
    "    if targets is not None:\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B*T, C)  # B*T shows T is useless outside of the attention module\n",
    "      targets = targets.view(B*T)\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "    else:\n",
    "      loss = None\n",
    "    \n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # learning: I need to start with the form of generation. In this case, the form is that of ChatGPT\n",
    "    # I provide a prompt (i.e., idx) and the model use that to generate texts of length max_new_tokens\n",
    "    for _ in range(max_new_tokens):\n",
    "      logits, _ = self(idx)\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      idx_next = torch.multinomial(probs, num_samples=1)\n",
    "      # learning: idx.append(idx_next) -> AttributeError: 'Tensor' object has no attribute 'append'\n",
    "      idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "# learning: the out.shape is [4,8,65] because we haven't done multinomial on the 65 logits yet. Therefore,\n",
    "# we have the last dimension of 65\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "# Use `torch.zeros((1,1), dtype=torch.long)` as the prompt. `torch.long` is necessary because the default \n",
    "# dtype is float with `zeros`\n",
    "print(m.generate(torch.zeros((1,1), dtype=torch.long), 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Wll.\n",
      "h tme eccon plit he hes,\n",
      "Se ou wr o thinIRGr hakerme t ymove I co sh be tled s be wn.\n",
      "\n",
      "Fumy be sk win\n",
      "HEdnat wirs m?\n",
      "Sct,\n",
      "cherous.\n",
      "\n",
      "CAThenybuthiut kNad,exxjaged be uesoufoulitobimethaREa h b.NanowBy dsth ckw w thesl.\n",
      "\n",
      "Co w SThe-\n",
      "ito'e:\n",
      "KI,\n",
      "KI dig h isory gouto he t gug Wiky w'thievif. omy tst nokn t mecll--chod m ast ty II wamie thend h FRS:\n",
      "O,\n",
      "TofBpimy thece h d bVon;, jf-dbou tlor mprvishplisom;-ve:\n",
      "WhayrKZZGr ed'sethe buaw,\n",
      "FRETothelo.\n",
      "EONThndou AtinerCAND-\n",
      "nSe than be;\n",
      "Ke be'ere med fe\n"
     ]
    }
   ],
   "source": [
    "# train the bigram model with torch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters())\n",
    "\n",
    "for steps in range(10000):\n",
    "  xb, yb = get_batch('train', batch_size, block_size)\n",
    "  logits, loss = m(xb, yb)\n",
    "\n",
    "  loss.backward()\n",
    "  # learning: I put zero_grad() before step() and no learning occurred. `.data -= lr * .grad`, nothing happens\n",
    "  # when `.grad` is 0\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "print(m.generate(torch.zeros((1,1), dtype=torch.long), 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8163, -0.9369],\n",
      "        [-0.3611,  0.4098],\n",
      "        [-1.3794,  0.2624],\n",
      "        [-1.2309,  0.4991],\n",
      "        [-0.9313,  1.2339],\n",
      "        [ 0.0413, -0.8217],\n",
      "        [-1.2193,  0.2188],\n",
      "        [-1.8425,  0.2470]])\n",
      "tensor([[-0.8163, -0.9369],\n",
      "        [-0.5887, -0.2635],\n",
      "        [-0.8523, -0.0882],\n",
      "        [-0.9469,  0.0586],\n",
      "        [-0.9438,  0.2937],\n",
      "        [-0.7796,  0.1078],\n",
      "        [-0.8424,  0.1237],\n",
      "        [-0.9674,  0.1391]])\n"
     ]
    }
   ],
   "source": [
    "# aggregate past context. The input and output share the same shape: (B, T, C)\n",
    "# note: there is a nested loop approach and a softmax approach to this\n",
    "# B,T,C = 4,8,2\n",
    "# x = torch.randn(B,T,C)\n",
    "# xbow = torch.zeros((B,T,C))\n",
    "# wei = torch.tril(torch.ones((T,T)))\n",
    "# aver = wei / torch.sum(wei, 1, keepdim=True)\n",
    "# xbow = aver @ x\n",
    "# print(x[0])\n",
    "# print(xbow[0])\n",
    "\n",
    "# the softmax approach\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # this wei is 2d\n",
    "aver = torch.softmax(wei, dim=-1)  # this is the \"attention pattern\" described in 1B3B video\n",
    "xbow = aver @ x # (T T) @ (T 2)\n",
    "print(x[0])\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern: \n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5632, 0.4368, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2522, 0.5212, 0.2266, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2067, 0.1677, 0.2689, 0.3566, 0.0000, 0.0000],\n",
      "        [0.2508, 0.1903, 0.2007, 0.1554, 0.2027, 0.0000],\n",
      "        [0.1366, 0.4121, 0.1002, 0.0900, 0.0245, 0.2366]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "value matrix before weighted sum (i.e., wei @ v):\n",
      "tensor([[-0.1455,  0.4547, -0.1619, -0.7530, -0.0438,  0.8191, -0.2321, -0.0998],\n",
      "        [ 0.0996, -0.0992,  0.1070,  0.3034, -1.0028,  0.4944,  0.1906, -0.7063],\n",
      "        [-0.8945, -1.5600,  0.6999, -0.2780,  0.6452, -0.9598,  0.1748,  0.4385],\n",
      "        [-0.6607, -1.7572,  1.0174,  0.4272,  0.1645, -0.4213,  0.1836,  0.8104],\n",
      "        [-0.1626,  0.3353, -0.0812, -0.9314,  0.5696,  0.4231, -0.7850,  0.1318],\n",
      "        [ 0.2839, -0.1994, -0.2856,  0.5389,  0.6558, -1.3646,  0.8218,  1.2218]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "output of the self attention head: \n",
      "tensor([[-0.1455,  0.4547, -0.1619, -0.7530, -0.0438,  0.8191, -0.2321, -0.0998],\n",
      "        [-0.0384,  0.2127, -0.0444, -0.2915, -0.4627,  0.6773, -0.0475, -0.3648],\n",
      "        [-0.1874, -0.2905,  0.1735, -0.0947, -0.3875,  0.2468,  0.0804, -0.2940],\n",
      "        [-0.4895, -0.9688,  0.5355, -0.0272,  0.0549, -0.1561,  0.0965,  0.2678],\n",
      "        [-0.3327, -0.4231,  0.2619, -0.3093,  0.0687,  0.1271, -0.1175,  0.0812],\n",
      "        [-0.0648, -0.3323,  0.1142,  0.1375, -0.1706, -0.1310,  0.2561,  0.1046]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# turn the dumb aggregate method into a single head of self attention\n",
    "B,T,C = 4,6,8\n",
    "head_size = 8\n",
    "x = torch.randn(B,T,C)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "q = query(x) # B, T, head_size\n",
    "k = key(x) # B, T, head_size\n",
    "v = value(x) # B, T, head_size\n",
    "wei = q @ k.transpose(-2, -1)  # this wei is 3d. that is why softmax's dim can't be 1, but is -1\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = torch.softmax(wei, dim=-1) # B, T, T\n",
    "print('attention pattern: ')\n",
    "print(wei[0])\n",
    "print('value matrix before weighted sum (i.e., wei @ v):')\n",
    "print(v[0])\n",
    "out = wei @ v  # B, T, head_size\n",
    "print('output of the self attention head: ')\n",
    "print(out[0])  # I can see how the weighted sum affects the output of the attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# adding a linear layer to the bigram model\n",
    "# adding a positional embedding to the model\n",
    "pos_embedding_table = nn.Embedding(8, 32)\n",
    "pos_embedding_table(torch.arange(8))\n",
    "print(torch.allclose(pos_embedding_table.weight, pos_embedding_table(torch.arange(8))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention definition and model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32\n",
    "block_size = 32\n",
    "n_embd = 256\n",
    "n_head = 4\n",
    "n_layer = 6\n",
    "max_iters = 10000\n",
    "eval_iters = 200\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "dropout = 0.2\n",
    "\n",
    "class Head(nn.Module):\n",
    "  \"\"\" One head of self attention \"\"\"\n",
    "  def __init__(self, head_size, n_embd):\n",
    "    super().__init__()\n",
    "    self.n_embd = n_embd\n",
    "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "    # learning: I forgot to use register_buffer. I use it because I don't want it to be in the computational graph\n",
    "    self.register_buffer('tril', torch.tril(torch.ones((block_size, block_size))))\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B,T,C = x.shape\n",
    "    # learning: what I wrote vs. what is enough: B, T, C = x.shape[0], x.shape[1], x.shape[2]\n",
    "    q = self.query(x)\n",
    "    k = self.key(x)\n",
    "    v = self.value(x)\n",
    "    # learning: the normalization base is C, I used 2\n",
    "    wei = q @ k.transpose(-2, -1) * self.n_embd**-0.5\n",
    "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "    wei = torch.softmax(wei, dim=-1)\n",
    "    wei = self.dropout(wei)\n",
    "    out = wei @ v\n",
    "    return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  \"\"\" Multiple heads of self attention in parallel \"\"\"\n",
    "  def __init__(self, n_head, n_embd):\n",
    "    super().__init__()\n",
    "    head_size = n_embd // n_head\n",
    "    # self.h = Head(head_size)\n",
    "    # learning: the above was what I did. I should instead put these Heads in a ModuleList\n",
    "    self.heads = nn.ModuleList([Head(head_size, n_embd) for _ in range(n_head)])\n",
    "    self.proj = nn.Linear(head_size * n_head, n_embd, bias=False)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # return torch.cat((self.h(x), self.h(x), self.h(x), self.h(x)), dim=-1)\n",
    "    # learning: the above was what I did. I should instead put these Heads in a ModuleList\n",
    "    # Because this class is a representation of MULTI heads instead of a single head being forwarded multiple times\n",
    "    x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    x = self.proj(x)\n",
    "    x = self.dropout(x)\n",
    "    return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "  \"\"\" This layer is applied to each position in a block locally and independently \"\"\"\n",
    "  def __init__(self, n_embd):\n",
    "    super().__init__()\n",
    "    # learning: I forgot the non-linearity. With the non-linearity, I also need nn.Sequential\n",
    "    self.net = nn.Sequential(\n",
    "      nn.Linear(n_embd, n_embd, bias=False),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(n_embd, n_embd, bias=False),  # projection layer\n",
    "      nn.Dropout(dropout)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.net(x)\n",
    "  \n",
    "class Block(nn.Module):\n",
    "  \"\"\" Transformer block: Communicate-then-compute\"\"\"\n",
    "\n",
    "  def __init__(self, n_head, n_embd):\n",
    "    super().__init__()\n",
    "    self.sa = MultiHeadAttention(n_head, n_embd)\n",
    "    self.ffwd = FeedForward(n_embd=n_embd)\n",
    "    self.ln1 = nn.LayerNorm(n_embd)\n",
    "    self.ln2 = nn.LayerNorm(n_embd)    \n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = x + self.sa(self.ln1(x))\n",
    "    x = x + self.ffwd(self.ln2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.token_embd_table = nn.Embedding(vocab_size, n_embd)\n",
    "    self.pos_embd_table = nn.Embedding(block_size, n_embd)\n",
    "    self.blocks = nn.Sequential(*[Block(n_head, n_embd) for _ in range(n_layer)])\n",
    "    self.ln_f = nn.LayerNorm(n_embd)\n",
    "    self.fully_connected = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    B, T = idx.shape\n",
    "    token_embd = self.token_embd_table(idx) # B T C\n",
    "    # learning: I wrote self.pos_embd_table(idx). But the pos_embd_table takes POSITION as input instead of idx\n",
    "    # learning 2: note that torch.arange(T) uses T instead of block_size because idx might be shorter than block_size\n",
    "    pos_embd = self.pos_embd_table(torch.arange(T, device=device)) # B T C\n",
    "    x = token_embd + pos_embd\n",
    "    x = self.blocks(x) # B T C\n",
    "    x = self.ln_f(x)\n",
    "    logits = self.fully_connected(x) # B T vocab_size\n",
    "\n",
    "    if targets is not None: # training mode\n",
    "      logits = logits.view(B*T, -1)\n",
    "      # learning: I wrote targets.view(B*T, -1), which returns a 2d tensor. I need 1d for targets instead\n",
    "      targets = targets.view(B*T)\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "    else:\n",
    "      loss = None\n",
    "\n",
    "    return logits, loss\n",
    "\n",
    "\n",
    "  def generate(self, idx, max_token):\n",
    "    for _ in range(max_token):\n",
    "      # learning: forgot to crop idx\n",
    "      idx_cond = idx[:, -block_size:]\n",
    "      logits, _ = self(idx_cond)\n",
    "      # learning: forgot the line below. Logits are generated for each and every position in a sequence and \n",
    "      # I only need the last set of logits\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      next_token = torch.multinomial(probs, num_samples=1)\n",
    "      idx = torch.cat((idx, next_token), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2407489 parameters\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# learning: parameters can be extracted simply using m\n",
    "m = GPTLanguageModel()\n",
    "optimizer = torch.optim.AdamW(m.parameters(), learning_rate)\n",
    "# learning: m.parameters() returns an iterable of params, each of which are defined \n",
    "# using basic layer constructs provided in torch.nn\n",
    "print(sum(p.numel() for p in m.parameters()), 'parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st training run attempt - failed\n",
    "```\n",
    "OUTPUT\n",
    "      0/   5000: 4.7011\n",
    "    200/   5000: 4.7024\n",
    "    400/   5000: 4.5861\n",
    "    600/   5000: 4.8061\n",
    "    800/   5000: 4.7599\n",
    "   1000/   5000: 4.8209\n",
    "   1200/   5000: 4.9178\n",
    "   1400/   5000: 4.7532\n",
    "   1600/   5000: 4.8195\n",
    "   1800/   5000: 4.6576\n",
    "   2000/   5000: 4.8024\n",
    "   2200/   5000: 4.6016\n",
    "   2400/   5000: 4.6853\n",
    "   2600/   5000: 4.7858\n",
    "   2800/   5000: 4.8272\n",
    "   3000/   5000: 4.8139\n",
    "   3200/   5000: 4.7915\n",
    "   3400/   5000: 4.6659\n",
    "   3600/   5000: 4.7886\n",
    "   3800/   5000: 4.7374\n",
    "   4000/   5000: 4.7108\n",
    "   4200/   5000: 4.8204\n",
    "   4400/   5000: 4.8655\n",
    "   4600/   5000: 4.7810\n",
    "   4800/   5000: 4.6445\n",
    "\n",
    "bA&XUjnT$TZ?Nhe$Rz:bhYKcdQgNbYF?GOajb'pw?M-\n",
    ";w?SnYwPqKLQw?ydxodwcXcGbIYmNT,Y\n",
    "Kyp$VX'DgaNY ziDbSOiaX,aTq mKRs!y&'d\n",
    "I'etfyGaH\n",
    "XjJSGKYO,vQ&iUjch&mvmmZkaTTKWzXr\n",
    "bYF?BnIBOacdhWTYKVTSjmmuZj ;XVbYKeqzSYCWoYNYReOO$NOA.j\n",
    "\n",
    "qhahWXTYFxN!.Ya'kzq$qvmiqfoGnstMMcaOFf3SSaF;fRaKAOaGYFRGsWCFY?Iq!qfamN-WBy!AF!PHgXa$JUT;:&bSjOmKMKRYT3Cj RKWIaUWzeW$FOah&mxUATVmROtqh!&uKRgNdTXb;Tbs\n",
    ";MuT?&aAb;,YKRaXKmxc:eTX;GaHhWz,Oa$OOa\n",
    "Wc!UObGOGUXOTSrTTYUGMFYm;jWzYjaqOBNUJ&dFbWC-Tm\n",
    "WDTTjA\n",
    "isiOb;.WhqA\n",
    "TXON!wv\n",
    "IqeNIPUiXhinYsf.UN\n",
    "T, DmP\n",
    "```\n",
    "\n",
    "Two questions For the above training run:\n",
    "1. I don't think I need to Carefully initialize the Parameters like the Make more series anymore Because The definition of parameters in a layer is not done by hand. But what is the range of initial loss? I got above four.\n",
    "  AN: Andrej's: `step 0: train loss 4.4116, val loss 4.4022`. Same range.\n",
    "2. The training is not working Given that the loss is not decreasing. Why?\n",
    "  AN: I have the this in the previous cell..\n",
    "```\n",
    "optimizer = torch.optim.AdamW(m.parameters())\n",
    "m = GPTLanguageModel()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "locked"
    ],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2696, val loss 4.2719\n",
      "step 500: train loss 1.9175, val loss 2.0111\n",
      "step 1000: train loss 1.7461, val loss 1.8874\n",
      "step 1500: train loss 1.6702, val loss 1.8241\n",
      "step 2000: train loss 1.6206, val loss 1.7888\n",
      "step 2500: train loss 1.5859, val loss 1.7849\n",
      "step 3000: train loss 1.5668, val loss 1.7473\n",
      "step 3500: train loss 1.5538, val loss 1.7492\n",
      "step 4000: train loss 1.5273, val loss 1.7249\n",
      "step 4500: train loss 1.5181, val loss 1.6975\n",
      "step 5000: train loss 1.4951, val loss 1.6804\n",
      "step 5500: train loss 1.4849, val loss 1.6858\n",
      "step 6000: train loss 1.4815, val loss 1.6848\n",
      "step 6500: train loss 1.4644, val loss 1.6799\n",
      "step 7000: train loss 1.4598, val loss 1.6607\n",
      "step 7500: train loss 1.4497, val loss 1.6547\n",
      "step 8000: train loss 1.4528, val loss 1.6553\n",
      "step 8500: train loss 1.4372, val loss 1.6530\n",
      "step 9000: train loss 1.4372, val loss 1.6371\n",
      "step 9500: train loss 1.4310, val loss 1.6339\n",
      "step 9999: train loss 1.4242, val loss 1.6229\n",
      "\n",
      "A summier:\n",
      "That which persuade; if I hear be but it speless\n",
      "Upon others modemn's happinessity,\n",
      "And was not work my knight!\n",
      "\n",
      "Third Senators:\n",
      "Ay, is rembumble faiths, I pray theve hence, that broth seem casts mon\n",
      "much dar'd make eye these father.\n",
      "\n",
      "LADY ANNE:\n",
      "A heading a most eye. Oy, well!\n",
      "Dod night!\n",
      "Is brave against thou wrought so\n",
      "so will say\n",
      "Vers, that never begar wit: he. God say how.\n",
      "3 KING HENRY VI\n",
      "\n",
      "ISASALET:\n",
      "There is years! let him, hearts by\n",
      "in, what sway at thish somplant,\n",
      "Live they deep \n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            # learning: using sampling over evaluating against entire datasets for efficiency\n",
    "            X, Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out\n",
    "\n",
    "for i in range(max_iters):\n",
    "  xb, yb = get_batch('train', batch_size, block_size)\n",
    "  _, loss = m(xb, yb)\n",
    "  loss.backward()\n",
    "\n",
    "  if i % eval_interval == 0 or i == max_iters-1:\n",
    "    losses = estimate_loss()\n",
    "    print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "  # learning: forgot optimizer.step(). That is used to update the params\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, 500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results analysis\n",
    "### experiment 1\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 6\n",
    "n_embd = 32\n",
    "n_head = 4\n",
    "n_layer = 3\n",
    "max_iters = 5000\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "\n",
    "22849 parameters\n",
    "\n",
    "Results:\n",
    "- Training time: 25.1s\n",
    "- Performance: train loss 2.1236, val loss 2.1695\n",
    "\n",
    "### experiment 2\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 6\n",
    "n_embd = 32\n",
    "n_head = 4\n",
    "n_layer = 3\n",
    "max_iters = 10000 # increased\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "\n",
    "22849 parameters\n",
    "\n",
    "Results\n",
    "- Training time: 62.5s\n",
    "- Performance: train loss 2.0394, val loss 2.1237\n",
    "\n",
    "### experiment 3\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 32 # increased\n",
    "n_embd = 32\n",
    "n_head = 4\n",
    "n_layer = 3\n",
    "max_iters = 10000\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "\n",
    "23681 parameters\n",
    "\n",
    "Results:\n",
    "- Training time: 2m 36.3s\n",
    "- Performance: train loss 1.8381, val loss 1.9865\n",
    "\n",
    "### experiment 4\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 32\n",
    "n_embd = 32\n",
    "n_head = 4\n",
    "n_layer = 3\n",
    "max_iters = 10000\n",
    "learning_rate = 3e-4 # decreased from 1e-3\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "\n",
    "23681 parameters\n",
    "\n",
    "Results\n",
    "- Training time: 2m 36.3s\n",
    "- Performance: train loss 2.0735, val loss 2.1210\n",
    "\n",
    "Observation: This learning rate is too slow for the majority of training\n",
    "\n",
    "### experiment 5\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 32\n",
    "n_embd = 32\n",
    "n_head = 4\n",
    "n_layer = 6 # increased\n",
    "max_iters = 10000\n",
    "learning_rate = 1e-3\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "\n",
    "42113 parameters\n",
    "\n",
    "Results\n",
    "- Training time: 4m 58s\n",
    "- Performance: train loss 1.7351, val loss 1.8810\n",
    "\n",
    "Observation: The losses got below 2.0 with half of 10000 iterations\n",
    "\n",
    "### experiment 6\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 32\n",
    "n_embd = 128 # increased\n",
    "n_head = 4\n",
    "n_layer = 6\n",
    "max_iters = 10000\n",
    "learning_rate = 1e-3\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "\n",
    "610625 parameters\n",
    "\n",
    "Results\n",
    "- Training time: 8m 4.5s\n",
    "- Performance: train loss 1.4459, val loss 1.6770\n",
    "\n",
    "### experiment 7 (too long to train)\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 256 # increased\n",
    "n_embd = 128\n",
    "n_head = 4\n",
    "n_layer = 6\n",
    "max_iters = 10000\n",
    "learning_rate = 1e-3\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "\n",
    "639297 parameters\n",
    "\n",
    "Results\n",
    "- Training time: too long to train\n",
    "- Performance: too long to train\n",
    "\n",
    "Observation: this only increased the size of `pos_embd_table`. The parameters count didn't increase by much relatively speaking. However, Training takes significantly longer. It took 6 minutes to complete 500 iterations, that is 2hr estimated for 10000 iterations.\n",
    "\n",
    "### experiment 8: see if this is faster than set 7\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 32\n",
    "n_embd = 256 # increased\n",
    "n_head = 4\n",
    "n_layer = 6\n",
    "max_iters = 10000\n",
    "learning_rate = 1e-3\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "\n",
    "2,400,833 parameters (GPT3 has 175b params)\n",
    "\n",
    "Results\n",
    "- Training time: 14m 45s\n",
    "- Performance: train loss 1.4654, val loss 1.6836\n",
    "\n",
    "Observation: \n",
    "1. This is much faster than set 7. 500 iterations took about a minute. Seems like these params aren't created equal when it comes to impacts on training speed. Interestingly, the doubling of `n_embd` increased the parameter count by 4 folds while the increase of `block_size` only added about 30000 params to the count. \n",
    "2. There is no perf improvement over set 6. The second half of 10000 iterations didn't do much in terms of reducing the losses\n",
    "\n",
    "**Question: Why is the training time not proportional to the increase of param count?**\n",
    "AN (Claude): \n",
    "- The large increase in block_size (set 7) significantly slowed down training due to the quadratic complexity of attention mechanisms.\n",
    "- Increasing n_embd (set 8) increased the parameter count more but had a smaller impact on training time. This could be due to a combination of factors including better cache utilization and the M2's ability to handle larger matrix operations relatively efficiently.\n",
    "- the key point is the impact on training time depends more on how the parameters affect the computational structure of the model rather than just the raw number of parameters. \n",
    "\n",
    "### experiment 9: add layernorm to see if the second half of training works better\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 32\n",
    "n_embd = 256\n",
    "n_head = 4\n",
    "n_layer = 6\n",
    "max_iters = 10000\n",
    "learning_rate = 1e-3\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "3. layernorm\n",
    "\n",
    "2,407,489 parameters (GPT3 has 175b params)\n",
    "\n",
    "Results\n",
    "- Training time: 14m\n",
    "- Performance: train loss 1.3485, val loss 1.5908\n",
    "\n",
    "Observations: \n",
    "- I'm not sure if the second half of the training worked better but the training performance improved over experiment 8. Layernorm contributed to the overall training effectiveness.\n",
    "- overfitting is significant given the gap between train loss and val loss\n",
    "\n",
    "### experiment 10: add dropout to reduce overfitting\n",
    "```\n",
    "batch_size = 32\n",
    "block_size = 32\n",
    "n_embd = 256\n",
    "n_head = 4\n",
    "n_layer = 6\n",
    "max_iters = 10000\n",
    "learning_rate = 1e-3\n",
    "dropout = 0.2 # added\n",
    "```\n",
    "Arch: \n",
    "1. Multi headed attention\n",
    "2. residual connections\n",
    "3. layernorm\n",
    "4. dropout\n",
    "\n",
    "2,407,489 parameters (GPT3 has 175b params)\n",
    "\n",
    "Results\n",
    "- Training time: 17m 40s\n",
    "- Performance: train loss 1.4242, val loss 1.6229\n",
    "\n",
    "Observation: dropout seemed to maintain the gap between train loss and val loss. But it also seemed to hamper overall training performance. This run is worse than the previous even though overfitting is less severe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use GPU to train\n",
    "# TODO: review nanoGPT training script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
